=== Job 1162 started at Sat Nov 15 09:46:08 UTC 2025 on gpu-workers-0 ===
=== Running: flwr run . cluster-gpu ===
Loading project configuration... 
Success
/scratch/hackathon-venv/lib/python3.12/site-packages/joblib/_multiprocessing_helpers.py:44: UserWarning: [Errno 28] No space left on device.  joblib will operate in serial mode
  warnings.warn("%s.  joblib will operate in serial mode" % (e,))
[92mINFO [0m:      Device: AMD Instinct MI300X VF
[92mINFO [0m:      W&B disabled (credentials not provided). Set WANDB_API_KEY, WANDB_ENTITY, and WANDB_PROJECT to enable.
Downloading: "https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth" to /tmp/job-1162/xdg-cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth
  0%|          | 0.00/20.5M [00:00<?, ?B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 501MB/s]
[92mINFO [0m:      Starting HackathonFedAvg strategy:
[92mINFO [0m:      	â”œâ”€â”€ Number of rounds: 100
[92mINFO [0m:      	â”œâ”€â”€ ArrayRecord (15.49 MB)
[92mINFO [0m:      	â”œâ”€â”€ ConfigRecord (train): {'lr': 0.04}
[92mINFO [0m:      	â”œâ”€â”€ ConfigRecord (evaluate): (empty!)
[92mINFO [0m:      	â”œâ”€â”€> Sampling:
[92mINFO [0m:      	â”‚	â”œâ”€â”€Fraction: train (1.00) | evaluate ( 1.00)
[92mINFO [0m:      	â”‚	â”œâ”€â”€Minimum nodes: train (2) | evaluate (2)
[92mINFO [0m:      	â”‚	â””â”€â”€Minimum available nodes: 2
[92mINFO [0m:      	â””â”€â”€> Keys in records:
[92mINFO [0m:      		â”œâ”€â”€ Weighted by: 'num-examples'
[92mINFO [0m:      		â”œâ”€â”€ ArrayRecord key: 'arrays'
[92mINFO [0m:      		â””â”€â”€ ConfigRecord key: 'config'
[92mINFO [0m:      
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1/100]
[92mINFO [0m:      configure_train: Sampled 3 nodes (out of 3)
2025-11-15 09:46:20,670	WARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 0 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
[36m(ClientAppActor pid=1281392)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/joblib/_multiprocessing_helpers.py:44: UserWarning: [Errno 28] No space left on device.  joblib will operate in serial mode
[36m(ClientAppActor pid=1281392)[0m   warnings.warn("%s.  joblib will operate in serial mode" % (e,))
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[36m(ClientAppActor pid=1281393)[0m Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x73ada07882c0>
[36m(ClientAppActor pid=1281393)[0m Traceback (most recent call last):
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1689, in __del__
[36m(ClientAppActor pid=1281393)[0m     self._shutdown_workers()
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1620, in _shutdown_workers
[36m(ClientAppActor pid=1281393)[0m     if not self._shutdown:
[36m(ClientAppActor pid=1281393)[0m            ^^^^^^^^^^^^^^
[36m(ClientAppActor pid=1281393)[0m AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
[36m(ClientAppActor pid=1281393)[0m Epoch 1/2:   0%|          | 0/453 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/453 [00:00<?, ?it/s]
[36m(ClientAppActor pid=1281393)[0m Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x73ada07882c0>
[36m(ClientAppActor pid=1281393)[0m Traceback (most recent call last):
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1689, in __del__
[36m(ClientAppActor pid=1281393)[0m     self._shutdown_workers()
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1620, in _shutdown_workers
[36m(ClientAppActor pid=1281393)[0m     if not self._shutdown:
[36m(ClientAppActor pid=1281393)[0m            ^^^^^^^^^^^^^^
[36m(ClientAppActor pid=1281393)[0m AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[92mINFO [0m:      aggregate_train: Received 0 results and 3 failures
[92mINFO [0m:      	> Received error in reply from node 10202363704756031594: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device'>
[92mINFO [0m:      	> Received error in reply from node 10538855647740266198: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device'>
[92mINFO [0m:      	> Received error in reply from node 10524205022011639568: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 31, in train
    train_loss = train_fn(
                 ^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 316, in train
    for batch in progress_bar:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device'>
[92mINFO [0m:      configure_evaluate: Sampled 3 nodes (out of 3)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281392, ip=10.244.0.128, actor_id=256f40b2893faf6b8fa6cf6301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x75cff89a80b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281393, ip=10.244.0.128, actor_id=1f5120a4cd1d1d32dda1a7a601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x73b13dcb9af0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 124, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 189, in process_message
    raise ex
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 176, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/clientapp/client_app.py", line 161, in __call__
    return self._registered_funcs[full_name](message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/client_app.py", line 64, in evaluate
    eval_loss, tp, tn, fp, fn, probs, labels = test_fn(model, valloader, device)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/task.py", line 434, in test
    for batch in testloader:
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 499, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 432, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1149, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
                  ^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 169, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/usr/lib/python3.12/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1281394, ip=10.244.0.128, actor_id=7a8c0972f2361b7ed8eec15401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x755806c38ef0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.clientapp.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: [Errno 28] No space left on device

[91mERROR [0m:     ServerApp thread raised an exception: Message content is None. Use <message>.has_content() to check if a message has content.
[91mERROR [0m:     Traceback (most recent call last):
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/run_simulation.py", line 284, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/run_serverapp.py", line 62, in run
    server_app(grid=grid, context=context)
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/server_app.py", line 176, in __call__
    self._main(grid, context)
  File "/scratch/team04/1162/repo/cold_start_hackathon/server_app.py", line 67, in main
    result = strategy.start(
             ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/serverapp/strategy/strategy.py", line 254, in start
    agg_evaluate_metrics = self.aggregate_evaluate(
                           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/server_app.py", line 95, in aggregate_evaluate
    agg_metrics = compute_aggregated_metrics(replies)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/util.py", line 40, in compute_aggregated_metrics
    all_probs = [p for r in replies for p in r.content["metrics"]["probs"]]
                                             ^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/common/message.py", line 236, in content
    raise ValueError(
ValueError: Message content is None. Use <message>.has_content() to check if a message has content.

Exception in thread Thread-1 (server_th_with_start_checks):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/run_simulation.py", line 284, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/run_serverapp.py", line 62, in run
    server_app(grid=grid, context=context)
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/server/server_app.py", line 176, in __call__
    self._main(grid, context)
  File "/scratch/team04/1162/repo/cold_start_hackathon/server_app.py", line 67, in main
    result = strategy.start(
             ^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/serverapp/strategy/strategy.py", line 254, in start
    agg_evaluate_metrics = self.aggregate_evaluate(
                           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/server_app.py", line 95, in aggregate_evaluate
    agg_metrics = compute_aggregated_metrics(replies)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/team04/1162/repo/cold_start_hackathon/util.py", line 40, in compute_aggregated_metrics
    all_probs = [p for r in replies for p in r.content["metrics"]["probs"]]
                                             ^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/common/message.py", line 236, in content
    raise ValueError(
ValueError: Message content is None. Use <message>.has_content() to check if a message has content.
[36m(ClientAppActor pid=1281394)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/joblib/_multiprocessing_helpers.py:44: UserWarning: [Errno 28] No space left on device.  joblib will operate in serial mode[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=1281394)[0m   warnings.warn("%s.  joblib will operate in serial mode" % (e,))[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x73ada07882c0>[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m Traceback (most recent call last):[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1689, in __del__[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m     self._shutdown_workers()[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m   File "/scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1620, in _shutdown_workers[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m     if not self._shutdown:[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m            ^^^^^^^^^^^^^^[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=1281392)[0m Epoch 1/2:   0%|          | 0/429 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/429 [00:00<?, ?it/s][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=1281393)[0m Training on device: cuda:0
[36m(ClientAppActor pid=1281393)[0m Loaded /home/team04/xray-data/xray_fl_datasets_preprocessed_224/HospitalB/train
[36m(ClientAppActor pid=1281394)[0m Training on device: cuda:0[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ClientAppActor pid=1281393)[0m Loaded /home/team04/xray-data/xray_fl_datasets_preprocessed_224/HospitalC/eval[32m [repeated 5x across cluster][0m
Traceback (most recent call last):
  File "/scratch/hackathon-venv/bin/flower-simulation", line 8, in <module>
    sys.exit(run_simulation_from_cli())
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/run_simulation.py", line 156, in run_simulation_from_cli
    _ = _run_simulation(
        ^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/run_simulation.py", line 523, in _run_simulation
    updated_context = _main_loop(*args)
                      ^^^^^^^^^^^^^^^^^
  File "/scratch/hackathon-venv/lib/python3.12/site-packages/flwr/simulation/run_simulation.py", line 421, in _main_loop
    raise RuntimeError("Exception in ServerApp thread")
RuntimeError: Exception in ServerApp thread
Command '['flower-simulation', '--app', '.', '--num-supernodes', '3', '--backend-config', '{"client-resources": {"num-cpus": 2, "num-gpus": 0.33}}']' returned non-zero exit status 1.
=== Copying best model ===
=== Job 1162 finished with exit code 0 ===
