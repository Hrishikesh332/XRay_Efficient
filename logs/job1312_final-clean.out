=== Job 1312 started at Sat Nov 15 11:22:35 UTC 2025 on gpu-workers-1 ===
=== Running: flwr run . cluster-gpu ===
Loading project configuration... 
Success
[92mINFO [0m:      Device: AMD Instinct MI300X VF
[92mINFO [0m:      W&B disabled (credentials not provided). Set WANDB_API_KEY, WANDB_ENTITY, and WANDB_PROJECT to enable.
Downloading: "https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth" to /tmp/job-1312/xdg-cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth
  0%|          | 0.00/20.5M [00:00<?, ?B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 346MB/s]
[92mINFO [0m:      Starting HackathonFedAvg strategy:
[92mINFO [0m:      	â”œâ”€â”€ Number of rounds: 100
[92mINFO [0m:      	â”œâ”€â”€ ArrayRecord (15.49 MB)
[92mINFO [0m:      	â”œâ”€â”€ ConfigRecord (train): {'lr': 0.03}
[92mINFO [0m:      	â”œâ”€â”€ ConfigRecord (evaluate): (empty!)
[92mINFO [0m:      	â”œâ”€â”€> Sampling:
[92mINFO [0m:      	â”‚	â”œâ”€â”€Fraction: train (1.00) | evaluate ( 1.00)
[92mINFO [0m:      	â”‚	â”œâ”€â”€Minimum nodes: train (2) | evaluate (2)
[92mINFO [0m:      	â”‚	â””â”€â”€Minimum available nodes: 2
[92mINFO [0m:      	â””â”€â”€> Keys in records:
[92mINFO [0m:      		â”œâ”€â”€ Weighted by: 'num-examples'
[92mINFO [0m:      		â”œâ”€â”€ ArrayRecord key: 'arrays'
[92mINFO [0m:      		â””â”€â”€ ConfigRecord key: 'config'
[92mINFO [0m:      
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1/100]
[92mINFO [0m:      configure_train: Sampled 3 nodes (out of 3)
[36m(ClientAppActor pid=3182888)[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=3182888) is multi-threaded, use of fork() may lead to deadlocks in the child.
[36m(ClientAppActor pid=3182888)[0m   self.pid = os.fork()
[36m(ClientAppActor pid=3182888)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)
[36m(ClientAppActor pid=3182888)[0m   return data.pin_memory(device)
[36m(ClientAppActor pid=3182888)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)
[36m(ClientAppActor pid=3182888)[0m   return data.pin_memory(device)
[36m(ClientAppActor pid=3182888)[0m MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutions] Solver <ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC>, workspace required: 9728, provided ptr: 0 size: 0
[36m(ClientAppActor pid=3182886)[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=3182886) is multi-threaded, use of fork() may lead to deadlocks in the child.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ClientAppActor pid=3182886)[0m   self.pid = os.fork()[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=3182886)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=3182886)[0m   return data.pin_memory(device)[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=3182886)[0m /scratch/hackathon-venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=3182887)[0m /scratch/team04/1312/repo/cold_start_hackathon/task.py:283: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[36m(ClientAppActor pid=3182887)[0m   scheduler.step()
